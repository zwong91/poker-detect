{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import SAM\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    global point\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        point = (x, y)\n",
    "        print(f\"Point selected: {point}\")\n",
    "\n",
    "\n",
    "# Load a model\n",
    "model = SAM(\"sam2_t.pt\")\n",
    "\n",
    "video_path = fr\"videos/IMG_0036.mp4\"\n",
    "target_path = video_path.replace(\".mp4\", \"_segmented.mp4\")\n",
    "frame_generator = sv.get_video_frames_generator(source_path=video_path)\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=video_path)\n",
    "\n",
    "mask_annotator = sv.PolygonAnnotator()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "labeler = sv.LabelAnnotator()\n",
    "\n",
    "# Initialize global variables to store the point coordinates\n",
    "point = None\n",
    "\n",
    "\n",
    "cv2.namedWindow('Predictions')\n",
    "cv2.setMouseCallback('Predictions', click_event)\n",
    "\n",
    "# Open the video sink\n",
    "with sv.VideoSink(target_path=target_path, video_info=video_info) as sink:\n",
    "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
    "\n",
    "        if point is not None:\n",
    "            # Use the selected point for processing\n",
    "            results = model(frame, points=[point], labels=[1])[0]\n",
    "            masks = results.masks.data.cpu().numpy()\n",
    "            xyxy = sv.mask_to_xyxy(masks)\n",
    "            cls = np.array([0] * len(xyxy))\n",
    "            detections = sv.Detections(xyxy, masks, class_id=cls)\n",
    "\n",
    "            # Annotate the frame with boxes, masks, and labels\n",
    "            frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "            frame = mask_annotator.annotate(scene=frame, detections=detections)\n",
    "\n",
    "        # Display frame and wait for a click event\n",
    "        cv2.imshow('Predictions', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "\n",
    "        # Break the loop if the 'q' key is pressed\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with TensorRT 使用 TensorRT 运行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable MAX Power Mode and Jetson Clocks\n",
    "!sudo nvpmodel -m 0\n",
    "!sudo jetson_clocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor System Utilization\n",
    "!sudo apt update\n",
    "!sudo pip install jetson-stats\n",
    "!sudo reboot\n",
    "!jtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in /Users/es/miniconda3/envs/ultralytics/lib/python3.10/site-packages (0.5.4)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement sort (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for sort\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imutils sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Kalman filtering 卡尔曼滤波器\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sort'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import imutils\n",
    "from ultralytics import YOLO\n",
    "# Kalman filtering 卡尔曼滤波器\n",
    "from sort import *\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "torch.cuda.set_device(0)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "model = YOLO('best_prep.pt').to(device)\n",
    "\n",
    "video_path = \"videos/IMG_0036.MOV\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# https://medium.com/@mosesdaudu001/object-detection-tracking-with-yolov8-and-sort-algorithm-363be8bc0806\n",
    "sort_tracker = Sort(max_age=20, min_hits=2, iou_threshold=0.3)\n",
    "\n",
    "isFirstFrame = True\n",
    "\n",
    "t1 = time.time()\n",
    "fc = 0\n",
    "while True:\n",
    "\tret, frame = cap.read()\n",
    "\tif not ret:\n",
    "\t\tbreak\n",
    "\tfc = fc + 1\n",
    "\n",
    "\tif(isFirstFrame):\n",
    "\t\tisFirstFrame = False\n",
    "\n",
    "\tresults = model(frame)\n",
    "\n",
    "\tdets_to_sort = np.empty((0, 6))\n",
    "\tfor result in results:\n",
    "\t\tfor obj in result.boxes:\n",
    "\t\t\tbbox = obj.xyxy[0].cpu().numpy().astype(int)\n",
    "\t\t\tx1, y1, x2, y2 = bbox\n",
    "\n",
    "\t\t\tconf = obj.conf.item()\n",
    "\t\t\tclass_id = int(obj.cls.item())\n",
    "\t\t\tdets_to_sort = np.vstack((dets_to_sort, np.array([x1, y1, x2, y2, conf, class_id])))\n",
    "\t\t\t# cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "\t# Sort tracker 对检测结果进行跟踪, 更新卡尔曼滤波器的状态\n",
    "\ttracked_dets = sort_tracker.update(dets_to_sort)\n",
    "\tfor det in tracked_dets:\n",
    "\t\tx1, y1, x2, y2 = [int(i) for i in det[:4]]\n",
    "\t\ttrack_id = int(det[8]) if det[8] is not None else 0\n",
    "\t\tclass_id = int(det[4])\n",
    "\t\tcv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 4)\n",
    "\t\tcv2.putText(frame, f\"{track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
    "\n",
    "\t# 预测后的两帧的上下相对位置关系比较\n",
    "\tframe = imutils.resize(frame, width=800)\n",
    "\t# cv2.imshow('Frame', frame)\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == ord('q'):\n",
    "\t\tbreak\n",
    "\tif key == ord('p'):\n",
    "\t\tcv2.waitKey(-1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "t2 = time.time()\n",
    "ft = t2 - t1\n",
    "print(fc)\n",
    "print('Execution time {}'.format(ft))\n",
    "print('FPS: {}'.format(fc / ft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化YOLOv8预处理和后处理时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Batch Processing 批处理：如果要处理多个图像，将它们一起批处理可以显着减少每个图像的开销。您可以在predict调用中调整batch参数：\n",
    "\n",
    "results = model.predict([\"cat.jpg\", \"dog.jpg\"], batch=2, conf=0.7)\n",
    "\n",
    "##2. Reduce Image Size 减小图像大小\n",
    "\n",
    "##3. 使用混合精度 \n",
    "results = model.predict(\"cat.jpg\", conf=0.7, half=True)\n",
    "\n",
    "##4. cProfile或line_profiler分析工具来识别代码中的耗时瓶颈\n",
    "\n",
    "##4. Parallel Processing处理多个图像"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
